{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import os\n","from sklearn.metrics import confusion_matrix\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n","import pandas as pd\n","import numpy as np\n","\n","import tensorflow as tf\n","from tensorflow.python.keras import layers, models\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from tensorflow import keras\n","from sklearn import preprocessing\n","from sklearn.preprocessing import OneHotEncoder\n","from own_functions import reading_in, pre_pros_y, tokenize_corpus, read_synth, add_together, ready_CNN\n","from sklearn.utils import shuffle\n","import mlflow\n","import tensorflow_addons as tfa\n","from tqdm import tqdm\n","import random\n","from sklearn.naive_bayes import GaussianNB\n"]},{"cell_type":"markdown","metadata":{},"source":["# Run CNNs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cat_lab = {'emoji': 'emoji','emojii': 'emoji',\n","\n","'code-switching': 'other languages','foreign language': 'other languages','different languages': 'other languages',\n","\n","'typos': 'spelling and punctuation','spelling': 'spelling and punctuation','spelling error': 'spelling and punctuation',\n","'misspelling': 'spelling and punctuation',\"{'typos'}\": 'spelling and punctuation',\"{'typos', 'punct'}\": 'spelling and punctuation',\n","'typos, expand contractions, remove punctuation': 'spelling and punctuation',\"{'punct'}\": 'spelling and punctuation','typo': 'spelling and punctuation',\n","\n","'irony': 'irony or sarcasm','irony/sarcasm': 'irony or sarcasm','sarcasm': 'irony or sarcasm','ironic': 'irony or sarcasm',\n","\n","'negation': 'negation','negation (not + pos_adj)': 'negation','negation at the end': 'negation','short sentence with negation + postive adjective': 'negation',\n","\n","'double negation': 'double negation','negated negatives': 'double negation',\n","\n","'double meaning': 'confusing','confusing': 'confusing','hard to interpret': 'confusing','slang': 'confusing','vague': 'confusing','basic': 'confusing',\n","\n","'strong words': 'exaggeration','exaggeration': 'exaggeration',\n","\n","'replacement': 'vocabulary','synonym': 'vocabulary','ambigious words': 'vocabulary','rare words': 'vocabulary','infrequent word': 'vocabulary',\n","\n","'sentiment change over time': 'sentiment change in text','sentiment over time': 'sentiment change in text','temporal': 'sentiment change in text','hopes vs reality': 'sentiment change in text',\n","'past and present thoughts': 'sentiment change in text','time change': 'sentiment change in text'\n","\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mlflow.set_experiment(\"NLP project - CNN without embeddings\")\n","\n","tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR) #Filter info and warning messages\n","random.seed(42)\n","\n","train = reading_in('classification/music_reviews_train.json.gz')\n","dev = reading_in('classification/music_reviews_dev.json.gz')\n","dif = reading_in(\"classification/all difficult cases.txt.gz\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["want = []\n","for i in dif:\n","    if i[\"category\"].lower().strip() in cat_lab.keys():\n","        want.append(i)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dif_x = [i['reviewText'] for i in want if \"reviewText\" in i.keys() and \"sentiment\" in i.keys()]\n","dif_y =  [i['sentiment'] for i in want if \"reviewText\" in i.keys() and \"sentiment\" in i.keys()]\n","dif_cat_notthisone = [i['category'].lower().strip() for i in want if \"reviewText\" in i.keys() and \"sentiment\" in i.keys()]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_x = [i['reviewText'] for i in train if \"reviewText\" in i.keys() and \"sentiment\" in i.keys()]\n","train_y =  [i['sentiment'] for i in train if \"reviewText\" in i.keys() and \"sentiment\" in i.keys()]\n","\n","dev_x = [i['reviewText'] if \"reviewText\" in i.keys() else \"\" for i in dev]\n","dev_y = [i['sentiment'] for i in dev]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["###split into pos and neg\n","train_x_pos = []\n","train_x_neg = []\n","train_y_pos = []\n","train_y_neg = []\n","for i in range(len(train_x)):\n","    if train_y[i] == \"positive\":\n","        train_x_pos.append(train_x[i])\n","        train_y_pos.append(train_y[i])\n","    else:\n","        train_x_neg.append(train_x[i])\n","        train_y_neg.append(train_y[i])\n","train_x_neg.extend(train_x_pos[0:len(train_x_neg)])\n","train_y_neg.extend(train_y_pos[0:len(train_y_neg)])\n","balanced_x_train_s = train_x_neg\n","balanced_y_train_s = train_y_neg"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["balanced_x_train, balanced_y_train = shuffle(balanced_x_train_s, balanced_y_train_s, random_state= 42)\n","dev_x, dev_y = shuffle(dev_x, dev_y, random_state = 42)\n","#Tokenize X data\n","balanced_x_train = tokenize_corpus(balanced_x_train)\n","dev_x = tokenize_corpus(dev_x)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["paths = [\"webscrape.csv\",\"wordnet.csv\",\"word2vec.csv\"]\n","deli = [\" \",\"\\t\",\"\\t\"]\n","\n","synthetic = dict()\n","names= [\"web\",\"net\",\"vec\"]\n","for x,i in enumerate(paths):\n","    synthetic[names[x]] = read_synth(i,deli[x])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Scrape10_X, Scrape10_y = add_together(synthetic['web'], 10000,balanced_x_train, balanced_y_train)\n","Scrape5_X, Scrape5_y = add_together(synthetic['web'], 5000,balanced_x_train, balanced_y_train)\n","Scrape50_X, Scrape50_y = add_together(synthetic['web'], 500,balanced_x_train, balanced_y_train)\n","Scrape12_X, Scrape12_y = add_together(synthetic['web'], 1250,balanced_x_train, balanced_y_train)\n","Scrape2_X, Scrape2_y = add_together(synthetic['web'], 2500,balanced_x_train, balanced_y_train)\n","Scrape7_X, Scrape7_y = add_together(synthetic['web'], 7500,balanced_x_train, balanced_y_train)\n","\n","Net10_X, Net10_y = add_together(synthetic['net'], 10000, balanced_x_train, balanced_y_train)\n","Net5_X, Net5_y = add_together(synthetic['net'], 5000, balanced_x_train, balanced_y_train)\n","Net7_X, Net7_y = add_together(synthetic['net'], 7500, balanced_x_train, balanced_y_train)\n","Net2_X, Net2_y = add_together(synthetic['net'], 2500, balanced_x_train, balanced_y_train)\n","Net12_X, Net12_y = add_together(synthetic['net'], 1250, balanced_x_train, balanced_y_train)\n","Net50_X, Net50_y = add_together(synthetic['net'], 500,balanced_x_train, balanced_y_train)\n","\n","Vec10_X, Vec10_y =add_together(synthetic['vec'],10000,balanced_x_train, balanced_y_train)\n","Vec5_X, Vec5_y = add_together(synthetic['vec'], 5000,balanced_x_train, balanced_y_train)\n","Vec7_X, Vec7_y = add_together(synthetic['vec'], 7500,balanced_x_train, balanced_y_train)\n","Vec2_X, Vec2_y = add_together(synthetic['vec'], 2500,balanced_x_train, balanced_y_train)\n","Vec12_X, Vec12_y = add_together(synthetic['vec'], 1250,balanced_x_train, balanced_y_train)\n","Vec50_X, Vec50_y = add_together(synthetic['vec'], 500,balanced_x_train, balanced_y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Training X, training y,  description\n","train_list = [\n","    [balanced_x_train[:500], balanced_y_train[:500] ,'original data 500'],\n","    [balanced_x_train[:1250], balanced_y_train[:1250] ,'original data 1250'],\n","    [balanced_x_train[:10000], balanced_y_train[:10000],'original data 10000'],\n","    [balanced_x_train[:5000], balanced_y_train[:5000], 'original data 5000'], \n","    [balanced_x_train[:7500], balanced_y_train[:7500], 'original data 7500'], \n","    [balanced_x_train[:2500], balanced_y_train[:2500], 'original data 2500'], \n","\n","    [Scrape10_X, Scrape10_y , 'WebScrape 10000'],\n","    [Scrape5_X, Scrape5_y,'WebScrape 5000' ],\n","    [Scrape50_X, Scrape50_y, 'WebScrape 500'],\n","    [Scrape12_X, Scrape12_y, 'WebScrape 1250'],\n","    [Scrape2_X, Scrape2_y, 'WebScrape 2500'],\n","    [Scrape7_X, Scrape7_y, 'WebScrape 7500' ],\n","\n","    [Net10_X, Net10_y, 'WordNet 10000'],\n","    [Net5_X, Net5_y , 'WordNet 5000'],\n","    [Net7_X, Net7_y, 'WordNet 7500' ],\n","    [Net2_X, Net2_y, 'WordNet 2500'],\n","    [Net12_X, Net12_y , 'WordNet 1250'],\n","    [Net50_X, Net50_y, 'WordNet 500'],\n","\n","    [Vec10_X, Vec10_y, 'Word2Vec 10000' ],\n","    [Vec5_X, Vec5_y, 'Word2Vec 5000' ],\n","    [Vec7_X, Vec7_y, 'Word2Vec 7500' ],\n","    [Vec2_X, Vec2_y, 'Word2Vec 2500' ],\n","    [Vec12_X, Vec12_y, 'Word2Vec 1250' ],\n","    [Vec50_X, Vec50_y, 'Word2Vec 500' ]\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i in tqdm(train_list):\n","     with mlflow.start_run(run_name=f\"CNN WE with {i[2]}\"):\n","        \n","        tf.keras.backend.clear_session()\n","        tf.keras.backend.learning_phase()\n","        \n","        act = 'sigmoid'\n","        model = models.Sequential()\n","        model.add(layers.Conv1D(filters = 32, kernel_size = 10, strides = 5, padding = \"same\", activation='relu'))\n","        model.add(layers.AveragePooling1D(pool_size = 20, padding = \"same\", strides = 1))\n","        model.add(layers.Flatten())\n","        model.add(layers.Dense(256, activation=act))\n","        #model.add(layers.Dense(512, activation=act))\n","        #model.add(layers.Dense(1024, activation=act))\n","        model.add(layers.Dense(512, activation=act))\n","        model.add(layers.Dense(256, activation=act))\n","        model.add(layers.Dense(1, activation=act))\n","        \n","        \n","        #Get the data in the right format\n","        train_x, train_y, devt_x, devt_y, dif_temp_x, dif_temp_y = ready_CNN(i[0], i[1],dev_x, dev_y, dif_x, dif_y)\n","        model.compile(loss='binary_crossentropy', optimizer='adamax', metrics=['accuracy'])\n","        history = model.fit(train_x, train_y, epochs=10, batch_size = 500, callbacks=[callback], verbose=0) #change the numbers\n","        metrics = model.evaluate(devt_x, devt_y, verbose=0) #change the number\n","        mlflow.log_metric('binary_crossentropy', metrics[0])\n","        mlflow.log_metric('accuracy', metrics[1])\n","        mlflow.log_param('Actual epochs',len(history.history['loss']) )\n","        mlflow.log_param('NN size', '[256, 512, 256, 1, epoch = 10, betch = 500]')\n","        mlflow.log_param('activation', act)\n","        model.save(f'models/{i[2]}')\n","        \n","        #dataframes\n","        dev_pre = model.predict(devt_x)\n","        devt_y = np.reshape(devt_y, [len(devt_y),1])\n","        #print(dev_pre.shape)\n","        #dev_pre = dev_pre[:, 0, :]\n","        \n","        devt_y = np.transpose(devt_y, axes=None)\n","        dev_pre = np.transpose(dev_pre, axes=None)\n","        devt_y = devt_y.tolist()\n","        dev_pre = dev_pre.tolist()\n","        \n","        df = pd.DataFrame(list(zip(devt_y[0], dev_pre[0])),\n","               columns =['True', 'Predicted'])\n","    \n","        df.to_csv(f\"pics/CNN/dev {i[2]}.csv\")\n","        #################\n","        dif_pre = model.predict(dif_temp_x)\n","        dif_temp_y = np.reshape(dif_temp_y, [len(dif_temp_y),1])\n","        #dif_pre = dif_pre[:, 0, :]\n","        \n","        dif_temp_y = np.transpose(dif_temp_y, axes=None)\n","        dif_pre = np.transpose(dif_pre, axes=None)\n","        dif_temp_y = dif_temp_y.tolist()\n","        dif_pre = dif_pre.tolist()\n","        \n","        df = pd.DataFrame(list(zip(dif_temp_y[0], dif_pre[0], dif_cat_notthisone)),\n","               columns =['True', 'Predicted', 'Cat'])\n","    \n","        df.to_csv(f\"pics/CNN/dif {i[2]}.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["# Run NB"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def ready_NB(data_X, data_y, dev_x, dev_y, dif_x, dif_y):\n","    #ADD TOKENIZER TO THE DEVELEPMENT DATA\n","\n","    vectorizer = TfidfVectorizer()\n","    tfid = vectorizer.fit(data_X)\n","    train_x = tfid.transform(data_X).toarray()\n","    #train_x = np.reshape(train_x, (len(train_x), train_x.shape[1], 1))\n","    #train_x = tf.convert_to_tensor(train_x)\n","\n","    train_y = pre_pros_y(data_y)\n","    \n","    devt_x = tfid.transform(dev_x)\n","    devt_x = devt_x.toarray()\n","    #devt_x = np.reshape(devt_x, (len(devt_x), devt_x.shape[1], 1))\n","    #devt_x = tf.convert_to_tensor(devt_x)\n","\n","    devt_y = pre_pros_y(dev_y)\n","\n","    dif_x = tfid.transform(dif_x)\n","    dif_x = dif_x.toarray()\n","    #dif_x = np.reshape(dif_x, (len(dif_x), dif_x.shape[1], 1))\n","    #dif_x = tf.convert_to_tensor(dif_x)\n","\n","    dif_y = pre_pros_y(dif_y)\n","    return train_x, train_y, devt_x, devt_y, dif_x, dif_y"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["random.seed(2)\n","MNB = Pipeline([\n","    ('CountVectorizer',CountVectorizer(ngram_range=(1, 2), tokenizer = tokenize_corpus, lowercase = False )),\n","    ('Tfidf',TfidfTransformer()),\n","    ('clf', MultinomialNB(alpha=1, fit_prior=False))\n","])#best params given here"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i in tqdm(train_list):\n","     with mlflow.start_run(run_name=f\"NB WE with {i[2]}\"):\n","        \n","        #Get the data in the right format\n","        train_x, train_y, devt_x, devt_y, dif_temp_x, dif_temp_y = ready_NB(i[0], i[1],dev_x, dev_y, dif_x, dif_y)\n","        MNB.fit(train_x, train_y) #change the numbers\n","\n","        #dataframes\n","        dev_pre = model.predict(devt_x)\n","        dev_pre = np.transpose(dev_pre, axes=None)\n","        devt_y = np.transpose(devt_y, axes=None)\n","        df = pd.DataFrame()\n","        df['True'] = devt_y\n","        df['Predicted'] = dev_pre\n","    \n","        df.to_csv(f\"pics/NB/dev {i[2]}.csv\")\n","        #################\n","\n","        dif_pre = model.predict(dif_temp_x)\n","        dif_pre = np.transpose(dif_pre, axes=None)\n","        dif_temp_y = np.transpose(dif_temp_y, axes=None)\n","        df = pd.DataFrame()\n","        df['True'] = dif_temp_y\n","        df['Predicted'] = dif_pre\n","    \n","        df.to_csv(f\"pics/NB/dif {i[2]}.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["# Find train size"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def reading_in(path):\n","    thing=[]\n","    for line in gzip.open(path):\n","        review_data = json.loads(line)\n","        subthing = dict()\n","        for key in review_data:\n","            subthing[key]= review_data[key]\n","        thing.append(subthing)\n","    return thing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train = reading_in('../data/classification/music_reviews_train.json.gz')\n","dev = reading_in('../data/classification/music_reviews_dev.json.gz')\n","test = reading_in('../data/classification/music_reviews_test_masked.json.gz')\n","difficult = reading_in('../data/difficult cases/phase2_testData-masked.json.gz')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_x = [i['reviewText'] if \"reviewText\" in i.keys() else \"\" for i in train]\n","train_y =  [i['sentiment'] for i in train]\n","\n","dev_x = [i['reviewText'] if \"reviewText\" in i.keys() else \"\" for i in dev]\n","dev_y = [i['sentiment'] for i in dev]\n","\n","test_x = [i['reviewText'] if \"reviewText\" in i.keys() else \"\" for i in test]\n","test_y = [i['sentiment'] for i in test]\n","\n","diff_test_x = [i['reviewText'] if \"reviewText\" in i.keys() else \"\" for i in test]\n","diff_test_y = [i['sentiment'] for i in test] #empty"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pipeline = Pipeline([\n","    ('CountVectorizer',CountVectorizer(ngram_range=(1, 2))),\n","    ('Tfidf',TfidfTransformer()),\n","    ('clf', MultinomialNB(alpha=1, fit_prior=False))\n","])#best params given here"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def find_train_size():\n","    \"\"\"\n","    90% train, 10% dev (test)\n","    \"\"\"\n","    macro_f1 = []\n","    train_size = 1000\n","    \n","    while train_size <= len(train_x):\n","        \n","        temp_train_x = train_x[0:train_size]\n","        temp_train_y = train_y[0:train_size]\n","\n","        pipeline.fit(temp_train_x, temp_train_y)\n","        pred_dev = pipeline.predict(dev_x)\n","                \n","        macro_f1.append((train_size, f1_score(pred_dev, dev_y, average='macro')))\n","\n","        print(int(train_size))\n","        train_size += 1000\n","        \n","    return macro_f1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["macro_f1_dev= find_train_size()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = pd.DataFrame(macro_f1_dev, columns =['train_size', 'F1-macro (dev)'])\n","df.to_csv('macro_f1_dev.csv')"]},{"cell_type":"markdown","metadata":{},"source":["# Heat Maps"]},{"cell_type":"markdown","metadata":{},"source":["## CNN"]},{"cell_type":"markdown","metadata":{},"source":["def pre(df):\n","    end_df = []\n","    for i in df:\n","        pre_pro = []\n","        pre = i['Predicted'].tolist()\n","        for p in pre:\n","            if p > 0.5:\n","                pre_pro.append(1)\n","            else:\n","                pre_pro.append(0)\n","        i['Predicted'] = pre_pro\n","        end_df.append(i)\n","    return end_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dev = [ 'dev WordNet 500.csv', 'dev Word2Vec 500.csv','dev WebScrape 500.csv', 'dev original data 500.csv',\\\n","    'dev WordNet 1250.csv', 'dev Word2Vec 1250.csv',  'dev WebScrape 1250.csv',  'dev original data 1250.csv',\\\n"," 'dev WordNet 2500.csv',  'dev Word2Vec 2500.csv', 'dev WebScrape 2500.csv',  'dev original data 2500.csv', \\\n"," 'dev WordNet 5000.csv',  'dev Word2Vec 5000.csv',  'dev WebScrape 5000.csv',  'dev original data 5000.csv',\\\n"," 'dev WordNet 7500.csv',  'dev Word2Vec 7500.csv', 'dev WebScrape 7500.csv', 'dev original data 7500.csv',\\\n"," 'dev WordNet 10000.csv', 'dev Word2Vec 10000.csv', 'dev WebScrape 10000.csv', 'dev original data 10000.csv'\\\n","]\n","\n","dif =  [ 'dif WordNet 500.csv', 'dif Word2Vec 500.csv','dif WebScrape 500.csv', 'dif original data 500.csv',\\\n"," 'dif WordNet 1250.csv', 'dif Word2Vec 1250.csv',  'dif WebScrape 1250.csv',  'dif original data 1250.csv',\\\n"," 'dif WordNet 2500.csv',  'dif Word2Vec 2500.csv', 'dif WebScrape 2500.csv',  'dif original data 2500.csv', \\\n"," 'dif WordNet 5000.csv',  'dif Word2Vec 5000.csv',  'dif WebScrape 5000.csv',  'dif original data 5000.csv',\\\n"," 'dif WordNet 7500.csv',  'dif Word2Vec 7500.csv', 'dif WebScrape 7500.csv', 'dif original data 7500.csv',\\\n"," 'dif WordNet 10000.csv', 'dif Word2Vec 10000.csv', 'dif WebScrape 10000.csv', 'dif original data 10000.csv'\\\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dif_p = [pd.read_csv('pics/CNN'+i) for i in dif]\n","dev_p = [pd.read_csv('pics/CNN'+i) for i in dev]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dif_p = pre(dif_p)\n","dev_p = pre(dev_p)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, axes = plt.subplots(6, 4, figsize=(25, 25), sharey=True)\n","cmap = sns.color_palette(\"Blues\", as_cmap=True)\n","\n","\n","sns.heatmap(confusion_matrix(dif_p[0]['True'], dif_p[0]['Predicted']), cmap = cmap ,ax = axes[0,0], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[0][4:-4])\n","sns.heatmap(confusion_matrix(dif_p[1]['True'], dif_p[1]['Predicted']), cmap = cmap ,ax = axes[0,1], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[1][4:-4])\n","sns.heatmap(confusion_matrix(dif_p[2]['True'], dif_p[2]['Predicted']), cmap = cmap ,ax = axes[0,2], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[2][4:-4])\n","sns.heatmap(confusion_matrix(dif_p[3]['True'], dif_p[3]['Predicted']), cmap = cmap ,ax = axes[0,3], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[3][4:-4])\n","\n","sns.heatmap(confusion_matrix(dif_p[4]['True'], dif_p[4]['Predicted']), cmap = cmap ,ax = axes[1,0], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[4][4:-4])\n","sns.heatmap(confusion_matrix(dif_p[5]['True'], dif_p[5]['Predicted']), cmap = cmap ,ax = axes[1,1], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[5][4:-4])\n","sns.heatmap(confusion_matrix(dif_p[6]['True'], dif_p[6]['Predicted']), cmap = cmap ,ax = axes[1,2], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[6][4:-4])\n","sns.heatmap(confusion_matrix(dif_p[7]['True'], dif_p[7]['Predicted']), cmap = cmap ,ax = axes[1,3], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[7][4:-4])\n","\n","sns.heatmap(confusion_matrix(dif_p[8]['True'], dif_p[8]['Predicted']), cmap = cmap ,ax = axes[2,0], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[8][4:-4])\n","sns.heatmap(confusion_matrix(dif_p[9]['True'], dif_p[9]['Predicted']), cmap = cmap ,ax = axes[2,1], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[9][4:-4])\n","sns.heatmap(confusion_matrix(dif_p[10]['True'], dif_p[10]['Predicted']),cmap = cmap , ax = axes[2,2], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[10][4:-4])\n","sns.heatmap(confusion_matrix(dif_p[11]['True'], dif_p[11]['Predicted']),cmap = cmap , ax = axes[2,3], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[11][4:-4])\n","\n","sns.heatmap(confusion_matrix(dif_p[12]['True'], dif_p[12]['Predicted']),cmap = cmap , ax = axes[3,0], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[12][4:-4])\n","sns.heatmap(confusion_matrix(dif_p[13]['True'], dif_p[13]['Predicted']),cmap = cmap , ax = axes[3,1], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[13][4:-4])\n","sns.heatmap(confusion_matrix(dif_p[14]['True'], dif_p[14]['Predicted']),cmap = cmap , ax = axes[3,2], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[14][4:-4])\n","sns.heatmap(confusion_matrix(dif_p[15]['True'], dif_p[15]['Predicted']),cmap = cmap , ax = axes[3,3], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[15][4:-4])\n","\n","sns.heatmap(confusion_matrix(dif_p[16]['True'], dif_p[16]['Predicted']), cmap = cmap ,ax = axes[4,0], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[16][4:-4])\n","sns.heatmap(confusion_matrix(dif_p[17]['True'], dif_p[17]['Predicted']), cmap = cmap ,ax = axes[4,1], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[17][4:-4])\n","sns.heatmap(confusion_matrix(dif_p[18]['True'], dif_p[18]['Predicted']), cmap = cmap ,ax = axes[4,2], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[18][4:-4])\n","sns.heatmap(confusion_matrix(dif_p[19]['True'], dif_p[19]['Predicted']), cmap = cmap ,ax = axes[4,3], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[19][4:-4])\n","\n","sns.heatmap(confusion_matrix(dif_p[20]['True'], dif_p[20]['Predicted']),cmap = cmap , ax = axes[5,0], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[20][4:-4])\n","sns.heatmap(confusion_matrix(dif_p[21]['True'], dif_p[21]['Predicted']),cmap = cmap , ax = axes[5,1], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[21][4:-4])\n","sns.heatmap(confusion_matrix(dif_p[22]['True'], dif_p[22]['Predicted']),cmap = cmap , ax = axes[5,2], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[22][4:-4])\n","sns.heatmap(confusion_matrix(dif_p[23]['True'], dif_p[23]['Predicted']),cmap = cmap , ax = axes[5,3], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[23][4:-4])\n","\n","\n","plt.savefig('pics/maps/CNN/Difficult cases CNN')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, axes = plt.subplots(6, 4, figsize=(25, 25), sharey=True)\n","\n","sns.heatmap(confusion_matrix(dev_p[0]['True'], dev_p[0]['Predicted']), cmap = cmap ,ax = axes[0,0], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[0][4:-4])\n","sns.heatmap(confusion_matrix(dev_p[1]['True'], dev_p[1]['Predicted']), cmap = cmap ,ax = axes[0,1], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[1][4:-4])\n","sns.heatmap(confusion_matrix(dev_p[2]['True'], dev_p[2]['Predicted']), cmap = cmap ,ax = axes[0,2], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[2][4:-4])\n","sns.heatmap(confusion_matrix(dev_p[3]['True'], dev_p[3]['Predicted']), cmap = cmap ,ax = axes[0,3], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[3][4:-4])\n","\n","sns.heatmap(confusion_matrix(dev_p[4]['True'], dev_p[4]['Predicted']), cmap = cmap ,ax = axes[1,0], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[4][4:-4])\n","sns.heatmap(confusion_matrix(dev_p[5]['True'], dev_p[5]['Predicted']), cmap = cmap ,ax = axes[1,1], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[5][4:-4])\n","sns.heatmap(confusion_matrix(dev_p[6]['True'], dev_p[6]['Predicted']), cmap = cmap ,ax = axes[1,2], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[6][4:-4])\n","sns.heatmap(confusion_matrix(dev_p[7]['True'], dev_p[7]['Predicted']), cmap = cmap ,ax = axes[1,3], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[7][4:-4])\n","\n","sns.heatmap(confusion_matrix(dev_p[8]['True'], dev_p[8]['Predicted']), cmap = cmap ,ax = axes[2,0], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[8][4:-4])\n","sns.heatmap(confusion_matrix(dev_p[9]['True'], dev_p[9]['Predicted']), cmap = cmap ,ax = axes[2,1], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[9][4:-4])\n","sns.heatmap(confusion_matrix(dev_p[10]['True'], dev_p[10]['Predicted']),cmap = cmap , ax = axes[2,2], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[10][4:-4])\n","sns.heatmap(confusion_matrix(dev_p[11]['True'], dev_p[11]['Predicted']),cmap = cmap , ax = axes[2,3], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[11][4:-4])\n","\n","sns.heatmap(confusion_matrix(dev_p[12]['True'], dev_p[12]['Predicted']),cmap = cmap , ax = axes[3,0], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[12][4:-4])\n","sns.heatmap(confusion_matrix(dev_p[13]['True'], dev_p[13]['Predicted']),cmap = cmap , ax = axes[3,1], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[13][4:-4])\n","sns.heatmap(confusion_matrix(dev_p[14]['True'], dev_p[14]['Predicted']),cmap = cmap , ax = axes[3,2], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[14][4:-4])\n","sns.heatmap(confusion_matrix(dev_p[15]['True'], dev_p[15]['Predicted']),cmap = cmap , ax = axes[3,3], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[15][4:-4])\n","\n","sns.heatmap(confusion_matrix(dev_p[16]['True'], dev_p[16]['Predicted']), cmap = cmap ,ax = axes[4,0], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[16][4:-4])\n","sns.heatmap(confusion_matrix(dev_p[17]['True'], dev_p[17]['Predicted']), cmap = cmap ,ax = axes[4,1], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[17][4:-4])\n","sns.heatmap(confusion_matrix(dev_p[18]['True'], dev_p[18]['Predicted']), cmap = cmap ,ax = axes[4,2], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[18][4:-4])\n","sns.heatmap(confusion_matrix(dev_p[19]['True'], dev_p[19]['Predicted']), cmap = cmap ,ax = axes[4,3], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[19][4:-4])\n","\n","sns.heatmap(confusion_matrix(dev_p[20]['True'], dev_p[20]['Predicted']),cmap = cmap , ax = axes[5,0], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[20][4:-4])\n","sns.heatmap(confusion_matrix(dev_p[21]['True'], dev_p[21]['Predicted']),cmap = cmap , ax = axes[5,1], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[21][4:-4])\n","sns.heatmap(confusion_matrix(dev_p[22]['True'], dev_p[22]['Predicted']),cmap = cmap , ax = axes[5,2], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[22][4:-4])\n","sns.heatmap(confusion_matrix(dev_p[23]['True'], dev_p[23]['Predicted']),cmap = cmap , ax = axes[5,3], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[23][4:-4])\n","\n","\n","plt.savefig('pics/maps/CNN/Validation cases CNN')"]},{"cell_type":"markdown","metadata":{},"source":["## Naive Bayes"]},{"cell_type":"markdown","metadata":{},"source":["def pre(df):\n","    end_df = []\n","\n","    for i in df:\n","        print(i.columns)\n","        pre_pro = []\n","\n","        pre = i['Predicted'].tolist()\n","        for p in pre:\n","            if p == 'positive':\n","                pre_pro.append(1)\n","            else:\n","                pre_pro.append(0)\n","        \n","        true_pro = []\n","        true = i['True'].tolist()\n","        for p in true:\n","            if p == 'positive':\n","                true_pro.append(1)\n","            else:\n","                true_pro.append(0)\n","\n","        i['Predicted'] = pre_pro\n","        i['True'] = true_pro\n","        end_df.append(i)\n","    return end_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dev = [ 'dev WordNet 500.csv', 'dev Word2Vec 500.csv','dev WebScrape 500.csv', 'dev original data 500.csv',\\\n","    'dev WordNet 1250.csv', 'dev Word2Vec 1250.csv',  'dev WebScrape 1250.csv',  'dev original data 1250.csv',\\\n"," 'dev WordNet 2500.csv',  'dev Word2Vec 2500.csv', 'dev WebScrape 2500.csv',  'dev original data 2500.csv', \\\n"," 'dev WordNet 5000.csv',  'dev Word2Vec 5000.csv',  'dev WebScrape 5000.csv',  'dev original data 5000.csv',\\\n"," 'dev WordNet 7500.csv',  'dev Word2Vec 7500.csv', 'dev WebScrape 7500.csv', 'dev original data 7500.csv',\\\n"," 'dev WordNet 10000.csv', 'dev Word2Vec 10000.csv', 'dev WebScrape 10000.csv', 'dev original data 10000.csv'\\\n","]\n","\n","dif =  [ 'dif WordNet 500.csv', 'dif Word2Vec 500.csv','dif WebScrape 500.csv', 'dif original data 500.csv',\\\n"," 'dif WordNet 1250.csv', 'dif Word2Vec 1250.csv',  'dif WebScrape 1250.csv',  'dif original data 1250.csv',\\\n"," 'dif WordNet 2500.csv',  'dif Word2Vec 2500.csv', 'dif WebScrape 2500.csv',  'dif original data 2500.csv', \\\n"," 'dif WordNet 5000.csv',  'dif Word2Vec 5000.csv',  'dif WebScrape 5000.csv',  'dif original data 5000.csv',\\\n"," 'dif WordNet 7500.csv',  'dif Word2Vec 7500.csv', 'dif WebScrape 7500.csv', 'dif original data 7500.csv',\\\n"," 'dif WordNet 10000.csv', 'dif Word2Vec 10000.csv', 'dif WebScrape 10000.csv', 'dif original data 10000.csv'\\\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dif_p = [pd.read_csv('pics/NB/'+i) for i in dif]\n","dev_p = [pd.read_csv('pics/NB/'+i) for i in dev]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dif_p = pre(dif_p)\n","dev_p = pre(dev_p)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, axes = plt.subplots(6, 4, figsize=(25, 25), sharey=True)\n","fig.suptitle('Difficult cases')\n","cmap = sns.color_palette(\"Blues\", as_cmap=True)\n","\n","\n","sns.heatmap(confusion_matrix(dif_p[0]['True'], dif_p[0]['Predicted']), cmap = cmap ,ax = axes[0,0], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[0][4:-4])\n","sns.heatmap(confusion_matrix(dif_p[1]['True'], dif_p[1]['Predicted']), cmap = cmap ,ax = axes[0,1], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[1][4:-4])\n","sns.heatmap(confusion_matrix(dif_p[2]['True'], dif_p[2]['Predicted']), cmap = cmap ,ax = axes[0,2], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[2][4:-4])\n","sns.heatmap(confusion_matrix(dif_p[3]['True'], dif_p[3]['Predicted']), cmap = cmap ,ax = axes[0,3], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[3][4:-4])\n","\n","sns.heatmap(confusion_matrix(dif_p[4]['True'], dif_p[4]['Predicted']), cmap = cmap ,ax = axes[1,0], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[4][4:-4])\n","sns.heatmap(confusion_matrix(dif_p[5]['True'], dif_p[5]['Predicted']), cmap = cmap ,ax = axes[1,1], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[5][4:-4])\n","sns.heatmap(confusion_matrix(dif_p[6]['True'], dif_p[6]['Predicted']), cmap = cmap ,ax = axes[1,2], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[6][4:-4])\n","sns.heatmap(confusion_matrix(dif_p[7]['True'], dif_p[7]['Predicted']), cmap = cmap ,ax = axes[1,3], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[7][4:-4])\n","\n","sns.heatmap(confusion_matrix(dif_p[8]['True'], dif_p[8]['Predicted']), cmap = cmap ,ax = axes[2,0], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[8][4:-4])\n","sns.heatmap(confusion_matrix(dif_p[9]['True'], dif_p[9]['Predicted']), cmap = cmap ,ax = axes[2,1], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[9][4:-4])\n","sns.heatmap(confusion_matrix(dif_p[10]['True'], dif_p[10]['Predicted']),cmap = cmap , ax = axes[2,2], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[10][4:-4])\n","sns.heatmap(confusion_matrix(dif_p[11]['True'], dif_p[11]['Predicted']),cmap = cmap , ax = axes[2,3], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[11][4:-4])\n","\n","sns.heatmap(confusion_matrix(dif_p[12]['True'], dif_p[12]['Predicted']),cmap = cmap , ax = axes[3,0], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[12][4:-4])\n","sns.heatmap(confusion_matrix(dif_p[13]['True'], dif_p[13]['Predicted']),cmap = cmap , ax = axes[3,1], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[13][4:-4])\n","sns.heatmap(confusion_matrix(dif_p[14]['True'], dif_p[14]['Predicted']),cmap = cmap , ax = axes[3,2], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[14][4:-4])\n","sns.heatmap(confusion_matrix(dif_p[15]['True'], dif_p[15]['Predicted']),cmap = cmap , ax = axes[3,3], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[15][4:-4])\n","\n","sns.heatmap(confusion_matrix(dif_p[16]['True'], dif_p[16]['Predicted']), cmap = cmap ,ax = axes[4,0], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[16][4:-4])\n","sns.heatmap(confusion_matrix(dif_p[17]['True'], dif_p[17]['Predicted']), cmap = cmap ,ax = axes[4,1], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[17][4:-4])\n","sns.heatmap(confusion_matrix(dif_p[18]['True'], dif_p[18]['Predicted']), cmap = cmap ,ax = axes[4,2], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[18][4:-4])\n","sns.heatmap(confusion_matrix(dif_p[19]['True'], dif_p[19]['Predicted']), cmap = cmap ,ax = axes[4,3], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[19][4:-4])\n","\n","sns.heatmap(confusion_matrix(dif_p[20]['True'], dif_p[20]['Predicted']),cmap = cmap , ax = axes[5,0], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[20][4:-4])\n","sns.heatmap(confusion_matrix(dif_p[21]['True'], dif_p[21]['Predicted']),cmap = cmap , ax = axes[5,1], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[21][4:-4])\n","sns.heatmap(confusion_matrix(dif_p[22]['True'], dif_p[22]['Predicted']),cmap = cmap , ax = axes[5,2], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[22][4:-4])\n","sns.heatmap(confusion_matrix(dif_p[23]['True'], dif_p[23]['Predicted']),cmap = cmap , ax = axes[5,3], annot=True, fmt=\"d\", vmin = 0, vmax = 1000).set(title= dif[23][4:-4])\n","\n","\n","plt.savefig('pics/maps/NB/Difficult cases NB')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, axes = plt.subplots(6, 4, figsize=(25, 25), sharey=True)\n","fig.suptitle('Validation set')\n","\n","sns.heatmap(confusion_matrix(dev_p[0]['True'], dev_p[0]['Predicted']), cmap = cmap ,ax = axes[0,0], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[0][4:-4])\n","sns.heatmap(confusion_matrix(dev_p[1]['True'], dev_p[1]['Predicted']), cmap = cmap ,ax = axes[0,1], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[1][4:-4])\n","sns.heatmap(confusion_matrix(dev_p[2]['True'], dev_p[2]['Predicted']), cmap = cmap ,ax = axes[0,2], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[2][4:-4])\n","sns.heatmap(confusion_matrix(dev_p[3]['True'], dev_p[3]['Predicted']), cmap = cmap ,ax = axes[0,3], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[3][4:-4])\n","\n","sns.heatmap(confusion_matrix(dev_p[4]['True'], dev_p[4]['Predicted']), cmap = cmap ,ax = axes[1,0], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[4][4:-4])\n","sns.heatmap(confusion_matrix(dev_p[5]['True'], dev_p[5]['Predicted']), cmap = cmap ,ax = axes[1,1], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[5][4:-4])\n","sns.heatmap(confusion_matrix(dev_p[6]['True'], dev_p[6]['Predicted']), cmap = cmap ,ax = axes[1,2], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[6][4:-4])\n","sns.heatmap(confusion_matrix(dev_p[7]['True'], dev_p[7]['Predicted']), cmap = cmap ,ax = axes[1,3], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[7][4:-4])\n","\n","sns.heatmap(confusion_matrix(dev_p[8]['True'], dev_p[8]['Predicted']), cmap = cmap ,ax = axes[2,0], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[8][4:-4])\n","sns.heatmap(confusion_matrix(dev_p[9]['True'], dev_p[9]['Predicted']), cmap = cmap ,ax = axes[2,1], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[9][4:-4])\n","sns.heatmap(confusion_matrix(dev_p[10]['True'], dev_p[10]['Predicted']),cmap = cmap , ax = axes[2,2], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[10][4:-4])\n","sns.heatmap(confusion_matrix(dev_p[11]['True'], dev_p[11]['Predicted']),cmap = cmap , ax = axes[2,3], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[11][4:-4])\n","\n","sns.heatmap(confusion_matrix(dev_p[12]['True'], dev_p[12]['Predicted']),cmap = cmap , ax = axes[3,0], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[12][4:-4])\n","sns.heatmap(confusion_matrix(dev_p[13]['True'], dev_p[13]['Predicted']),cmap = cmap , ax = axes[3,1], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[13][4:-4])\n","sns.heatmap(confusion_matrix(dev_p[14]['True'], dev_p[14]['Predicted']),cmap = cmap , ax = axes[3,2], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[14][4:-4])\n","sns.heatmap(confusion_matrix(dev_p[15]['True'], dev_p[15]['Predicted']),cmap = cmap , ax = axes[3,3], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[15][4:-4])\n","\n","sns.heatmap(confusion_matrix(dev_p[16]['True'], dev_p[16]['Predicted']), cmap = cmap ,ax = axes[4,0], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[16][4:-4])\n","sns.heatmap(confusion_matrix(dev_p[17]['True'], dev_p[17]['Predicted']), cmap = cmap ,ax = axes[4,1], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[17][4:-4])\n","sns.heatmap(confusion_matrix(dev_p[18]['True'], dev_p[18]['Predicted']), cmap = cmap ,ax = axes[4,2], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[18][4:-4])\n","sns.heatmap(confusion_matrix(dev_p[19]['True'], dev_p[19]['Predicted']), cmap = cmap ,ax = axes[4,3], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[19][4:-4])\n","\n","sns.heatmap(confusion_matrix(dev_p[20]['True'], dev_p[20]['Predicted']),cmap = cmap , ax = axes[5,0], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[20][4:-4])\n","sns.heatmap(confusion_matrix(dev_p[21]['True'], dev_p[21]['Predicted']),cmap = cmap , ax = axes[5,1], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[21][4:-4])\n","sns.heatmap(confusion_matrix(dev_p[22]['True'], dev_p[22]['Predicted']),cmap = cmap , ax = axes[5,2], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[22][4:-4])\n","sns.heatmap(confusion_matrix(dev_p[23]['True'], dev_p[23]['Predicted']),cmap = cmap , ax = axes[5,3], annot=True, fmt=\"d\", vmin = 0, vmax = 5000).set(title= dev[23][4:-4])\n","\n","\n","plt.savefig('pics/maps/NB/Validation cases')"]}],"metadata":{"language_info":{"name":"python"},"orig_nbformat":4,"deepnote_notebook_id":"8eedfb26-2394-4014-9e87-be240a411717"},"nbformat":4,"nbformat_minor":2}